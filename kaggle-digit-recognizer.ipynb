{
  "cells": [
    {
      "metadata": {
        "collapsed": false,
        "_cell_guid": "db1c60ac-b62a-47cb-a85b-6d251b98166b",
        "_uuid": "385975dae5c314684332fba49a79e745f62046e7",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf # TtensorFlow\nimport matplotlib.pyplot as plt # MATLAB-like plotting framework\n\n# Set ouput of plotting commands directly below the code cell that produced it.\n%matplotlib inline \n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "a223ea57-5758-4924-af7e-2722a87dd244",
        "_uuid": "5e43f6c72c935c893b952615e56f1cd6dc5b5e38",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Load the training dataset from train.csv. \n# Shuffle the examples and divide them into train, dev and test in the ratio 8:1:1.\n\nraw_dataset = pd.read_csv(\"../input/train.csv\").values\nnp.random.shuffle(raw_dataset)\nall_labels = raw_dataset[:, 0]\nall_features = raw_dataset[:, 1:] / 255 # Normalizes the pixel values.\n\nnum_examples = all_labels.size\na, b = num_examples * 8 // 10, num_examples * 9 // 10 # Ratios for the dataset split.\n\ndataset = {\n    'train_labels': all_labels[:a],\n    'dev_labels': all_labels[a:b],\n    'test_labels': all_labels[b:],\n    'train_features': all_features[:a, :],\n    'dev_features': all_features[a:b, :],\n    'test_features': all_features[b:, :]\n}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": false,
        "_cell_guid": "709d10b3-0f8d-4962-9531-08a59e93e400",
        "_uuid": "650a8adf35634f1f795bcbe669263c8115b58be5",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Check what a digit from the test_features division looks like:\n\ndef plot_digit(pixels):\n    plt.imshow(pixels.reshape(28, 28))\n    plt.show()\n    \nplot_digit(dataset['test_features'][0, :])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e191db8b-bc1f-4240-9df9-e527fae0ad5e",
        "_uuid": "475e50577c1570f4041c1a138ae06bda9a6a2fd9"
      },
      "cell_type": "markdown",
      "source": "**About the Model**\n\nThe model consists of a single hidden layer with 200 units using ReLU and an output layer using Softmax.\nIt has no regularization but uses the Adam optimizer and trains on the entire batch."
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "df9e90f0-1204-4189-8ec1-ad71535c5156",
        "_uuid": "edc624c4e4bf29d810834bbfec726ad261be778c",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Creates placeholder tensors for features and labels, as well as a set of 'Y' labels with\n# one-hot encoding to be used in costs calculation.\n\ndef input_nodes(num_features, num_labels):\n    X = tf.placeholder(tf.float32, shape = [None, num_features], name = 'X')\n    labels = tf.placeholder(tf.int64, shape = [None], name = 'labels')\n    \n    with tf.name_scope('labels_to_Y'):\n        Y = tf.one_hot(labels, num_labels, name = 'Y')\n        \n    return X, labels, Y\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "050e32ea-4a27-406a-a64d-233f082d3938",
        "_uuid": "87ef08bbbb4977a627630153086afd0bee232942",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Creates the Linear layer. Uses Xavier initializer for weight initialization.\n# This automatically determines the scale of initialization based on the number of input\n# and output neurons.\n\ndef linear_layer(input_, in_size, out_size, name):\n    with tf.variable_scope(name):\n        W = tf.get_variable('W', \n                            shape = [out_size, in_size], \n                            initializer = tf.contrib.layers.xavier_initializer())\n        b = tf.get_variable('b', \n                            shape = [out_size],\n                            initializer = tf.zeros_initializer())\n        \n    return tf.matmul(input_, W, transpose_b = True) + b",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "44f6e049-383d-4bce-991d-b3a9e0bb95ec",
        "_uuid": "384c130aa0847084ee08b39bf8cf147cba910b6b",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Creates the ReLU layer from a Linear layer created from the given inputs.\n\ndef relu_layer(input_, in_size, out_size, name):\n    return tf.nn.relu(linear_layer(input_, in_size, out_size, name))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "7050ebf7-97f1-4511-a1a4-dbbe6dccb2e9",
        "_uuid": "c69fa8f75a0d306004fef9df3e514e888268b06e",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Carries out forward propagation for each of the Linear layers,\n# then the final ReLU layer.\n\ndef forward_prop(X, layers):\n    for l in range(1, len(layers) - 1):\n        with tf.name_scope('relu_layer' + str(l)):\n            X = relu_layer(X, layers[l - 1], layers[l], 'weights' + str(l))\n    l = len(layers) - 1\n    with tf.name_scope('linear_layer'):\n        X = linear_layer(X, layers[l - 1], layers[l], 'weights' + str(l))\n    return X",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "1b304a6a-4c79-440e-b9ba-c96d018ce84d",
        "_uuid": "c1bdc813e11a9aed4b32cde000b0e1dd428b02a6",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Calculates the cost function for the given logits (predictions) and labels (actuals).\n# Uses softmax cross entropy as the measure of probability error, used in classification\n# tasks with mutually exclusive classes (can't be more than one digit).\n\ndef cost_function(logits, labels):\n    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits (logits = logits,\n                                                                  labels = labels))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "4cbd5c2c-7bd4-4816-8aec-724c16bdbe82",
        "_uuid": "60c30309f1dfe3c5bd32b36da3299470a6a19fef",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Returns the accuracy of the predictions.\n\ndef logits_to_labels(logits, num_labels):\n    return tf.argmax(logits, axis = 1)\n\ndef accuracy(predictions, labels):\n    correct = tf.equal(predictions, labels)\n    acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n    return acc",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "054f505a-f771-47b1-b383-a6d1e1a371af",
        "_uuid": "dd9717422c7308bb65f13c173bb8ab70b4d22ddf",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Create and train the model!\n\ndef model(X_train, labels_train, X_dev, labels_dev, layers = [784, 200, 10], num_epochs = 100, \n          learning_rate = 0.01, writer_dir = 'tensorboard/model1/default', \n          checkpoint = 'checkpoints/model.ckpt'):\n    tf.reset_default_graph()\n    \n    X, labels, Y = input_nodes(layers[0], layers[-1]) # placeholder tensors\n    \n    # Forward prop:\n    with tf.name_scope('forward_prop'):\n        logits = forward_prop(X, layers)\n    # Cost:\n    with tf.name_scope('cost'):\n        cost = cost_function(logits, Y)\n    # Accuracy of predictions:\n    with tf.name_scope('accuracy'):\n        acc = accuracy(logits_to_labels(logits, layers[-1]), labels)\n    # Summaries of cost and accuracy as scalar values:\n    with tf.name_scope('summaries'):\n        tf.summary.scalar('cost', cost)\n        tf.summary.scalar('accuracy', acc)\n        \n    # Constucts a new Adam optimizer and trains using the learning rate and cost function\n    with tf.name_scope('optimizer'):\n        train_step = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n        \n    # Merge all summaries collected in the default graph.\n    with tf.name_scope('summaries'):\n        merge = tf.summary.merge_all()\n        \n    init = tf.global_variables_initializer() # Op to initialize global variables in the graph\n    saver = tf.train.Saver() # Op to save and restore variables to and from checkpoints\n    \n    # Summary writers, create an event file for each dataset in a given directory \n    # and add summaries and events to it\n    graph_writer = tf.summary.FileWriter(writer_dir + '/graph')\n    train_writer = tf.summary.FileWriter(writer_dir + '/train')\n    dev_writer = tf.summary.FileWriter(writer_dir + '/dev')\n    \n    # Session setup:\n    with tf.Session() as sess:\n        graph_writer.add_graph(sess.graph)\n        sess.run(init)\n        \n        # Loop through the epochs:\n        for i in range(1, num_epochs + 1):\n            summary, _, cost_val = sess.run([merge, train_step, cost], {X: X_train, labels: labels_train})\n            train_writer.add_summary(summary, i)\n            if i % 10 == 0:\n                print('{}. iteration: train cost = {}'.format(i, cost_val))\n            \n            summary, cost_val = sess.run([merge, cost], { X: X_dev, labels: labels_dev })\n            dev_writer.add_summary(summary, i)\n            if i % 10 == 0:\n                print('dev cost = {}'.format(cost_val))\n        saver.save(sess, checkpoint)\n    \n    graph_writer.close()\n    train_writer.close()\n    dev_writer.close()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": false,
        "_cell_guid": "809d68e1-3bdc-4138-891b-da136c03f664",
        "_uuid": "99d17caaff90db25c6f02cfaa586eeb115d9d901",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Trains the model, calculating the costs on the training and dev sets:\n\nmodel(dataset['train_features'], dataset['train_labels'], dataset['dev_features'], \n      dataset['dev_labels'], num_epochs = 200, writer_dir = 'tensorboard/model1/1')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "c4d26232-d40d-45ab-bfe2-9c0981ed41e4",
        "_uuid": "e35a48413a4798df0198f4515a99bfd6a14aa396",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Generates a submission CSV containing predictions on the test dataset:\n\ndef generate_submission(checkpoint, layers):\n    tf.reset_default_graph()\n    challenge = pd.read_csv('../input/test.csv').values\n    X = tf.placeholder(tf.float32, [None, layers[0]])\n    logits = forward_prop(X, layers)\n    pred = logits_to_labels(logits, layers[-1])\n    saver = tf.train.Saver()\n    \n    with tf.Session() as sess:\n        saver.restore(sess, checkpoint)\n        pred = pred.eval({X: challenge})\n    df = pd.DataFrame(data = list(zip(range(1, pred.size+1), pred)), \n                      columns = ['ImageId', 'Label'])\n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": false,
        "_cell_guid": "3685c7a2-7b63-4c91-85d7-df338a98091b",
        "_uuid": "4d938bd58d21e2e87ed6184a370ce9c611c1cc47",
        "trusted": false
      },
      "cell_type": "code",
      "source": "df = generate_submission('checkpoints/model.ckpt', [784, 200, 10])\ndf.to_csv('submission.csv', index = False, header = True)\ndf",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "6e2c3e9c-cf23-497a-8d39-f68a6afb561d",
        "_uuid": "8d9721750eaa91068796efcb5af07d47797a7f45",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "nbconvert_exporter": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.6.3",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}